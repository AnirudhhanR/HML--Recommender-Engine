# -*- coding: utf-8 -*-
"""Copy of User Based Recommender Engine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S_E1vDaCRLvyN7OVQi50EEFUacpHaSs8
"""

#drive Link :

!pip install gdown pyunpack

! gdown 'https://drive.google.com/uc?id=1ZpdsOnCnoyBKQOVIxB5h9GAve3D05P7P'  #anime
! gdown 'https://drive.google.com/uc?id=1lmPQpgGHxXQz3WiEZ9Hla6EL58n34hrt'  #rating

#!pip install kaggle
#from google.colab import drive
#drive.mount('/content/drive')
#! mkdir ~/.kaggle
#!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json
#! chmod 600 ~/.kaggle/kaggle.json
#! kaggle datasets download CooperUnion/anime-recommendations-database

#! unzip /content/anime-recommendations-database.zip

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')

anime = pd.read_csv("/content/anime.csv")
rating = pd.read_csv("/content/rating.csv")

"""Basic Exploration"""

print(f"Shape of The Anime Dataset : {anime.shape}")
print(f"\nGlimpse of The Dataset :")
anime.head()#.style.set_properties(**{"background-color": "#2a9d8f","color":"white","border": "1.5px  solid black"})

"""Anime Dataset"""

print(f"Informations About Anime Dataset :\n")
print(anime.info())

"""rating dataset"""

print(f"Shape of The Rating Dataset : {rating.shape}")
print(f"\nGlimpse of The Dataset :")
rating.head()#.style.set_properties(**{"background-color": "#2a9d8f","color":"white","border": "1.5px  solid black"})

print(f"Informations About Rating Dataset :\n")
print(rating.info())

"""Dataset Summary

"""

print("Null Values of Anime Dataset :")
anime.isna().sum().to_frame().T.style#.set_properties(**{"background-color": "#2a9d8f","color":"white","border": "1.5px  solid black"})

print("After Dropping, Null Values of Anime Dataset :")
anime.dropna(axis = 0, inplace = True)
anime.isna().sum().to_frame().T.style#.set_properties(**{"background-color": "#2a9d8f","color":"white","border": "1.5px  solid black"})

dup_anime = anime[anime.duplicated()].shape[0]
print(f"There are {dup_anime} duplicate entries among {anime.shape[0]} entries in anime dataset.")

print(f"Summary of The Rating Dataset :")
rating.describe().T.style#.set_properties(**{"background-color": "#2a9d8f","color":"white","border": "1.5px  solid black"})

print("Null Values of Rating Dataset :")
rating.isna().sum().to_frame().T.style#.set_properties(**{"background-color": "#2a9d8f","color":"white","border": "1.5px  solid black"})

dup_rating = rating[rating.duplicated()].shape[0]
print(f"There are {dup_rating} duplicate entries among {rating.shape[0]} entries in rating dataset.")

rating.drop_duplicates(keep='first',inplace=True)
print(f"\nAfter removing duplicate entries there are {rating.shape[0]} entries in this dataset.")

"""merge the **anime** and **rating** datasets."""

fulldata = pd.merge(anime,rating,on="anime_id",suffixes= [None, "_user"])
fulldata = fulldata.rename(columns={"rating_user": "user_rating"})

print(f"Shape of The Merged Dataset : {fulldata.shape}")
print(f"\nGlimpse of The Merged Dataset :")

fulldata.head()#.style.set_properties(**{"background-color": "#2a9d8f","color":"white","border": "1.5px  solid black"})

"""Final Data Preprocessing

"""

data = fulldata.copy()
data["user_rating"].replace(to_replace = -1 , value = np.nan ,inplace=True)
data = data.dropna(axis = 0)
print("Null values after final pre-processing :")
data.isna().sum().to_frame().T.style#.set_properties(**{"background-color": "#2a9d8f","color":"white","border": "1.5px  solid black"})

selected_users = data["user_id"].value_counts()
data = data[data["user_id"].isin(selected_users[selected_users >= 50].index)]

data_pivot_temp = data.pivot_table(index="name",columns="user_id",values="user_rating").fillna(0)
data_pivot_temp.head()

"""Remove Special Charecters from Name"""

import re
def text_cleaning(text):
    text = re.sub(r'&quot;', '', text)
    text = re.sub(r'.hack//', '', text)
    text = re.sub(r'&#039;', '', text)
    text = re.sub(r'A&#039;s', '', text)
    text = re.sub(r'I&#039;', 'I\'', text)
    text = re.sub(r'&amp;', 'and', text)

    return text

data["name"] = data["name"].apply(text_cleaning)

data_pivot = data.pivot_table(index="name",columns="user_id",values="user_rating").fillna(0)
print("After Cleaning the animes names, let's see how it looks like.")
data_pivot.head()

"""Collaborative Recommender

"""

from scipy.sparse import csr_matrix
from sklearn.neighbors import NearestNeighbors

data_matrix = csr_matrix(data_pivot.values)
print(data_matrix.shape)

model_nn = NearestNeighbors(metric = "cosine", algorithm = "brute")
model_nn.fit(data_matrix)

#model_knn2 = KNeighborsClassifier(metric = "cosine", algorithm = "brute")
#model_knn2.fit()

var = 1
testdataframe = pd.DataFrame(columns = ['No', 'Anime Name', 'Rating'])
while var <= 3:
  query_no = np.random.choice(data_pivot.shape[0]) # random anime title and finding recommendation
  #print(f"We will find recommendation for {query_no} no anime which is {data_pivot.index[query_no]}.")
  distances, indices = model_knn.kneighbors(data_pivot.iloc[query_no,:].values.reshape(1, -1), n_neighbors = 11)
  print(distances)
  no = []
  name = []
  distance = []
  rating = []

  for i in range(0, len(distances.flatten())):
    if i == 0:
      print(f"Recommendations_for_{data_pivot.index[query_no]}_viewers\n")
    else:
         #  print(f"{i}: {data_pivot.index[indices.flatten()[i]]} , with distance of {distances.flatten()[i]}")
      no.append(i)
      name.append(data_pivot.index[indices.flatten()[i]])
      distance.append(distances.flatten()[i])
      rating.append(anime[anime["name"]==data_pivot.index[indices.flatten()[i]]]["rating"].values)

  dic = {"No" : no, "Anime Name" : name, "Rating" : rating}
  recommendation = pd.DataFrame(data = dic)
  recommendation.set_index("No", inplace = True)
  pd.concat([testdataframe,recommendation])
  print(recommendation)#.style.set_properties(**{"background-color": "#2a9d8f","color":"white","border": "1.5px  solid black"})
  print("\n\n")
  var = var + 1



